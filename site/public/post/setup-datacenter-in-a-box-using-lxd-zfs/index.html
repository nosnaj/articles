<!doctype html>
<html lang="en-US">
  <head>
    <title>Setup DataCenter In a Box Using LXD &#43; ZFS | EngineersMY</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">

    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:title" content="Setup DataCenter In a Box Using LXD &#43; ZFS | EngineersMY">
    <meta property="og:description" content="Community articles about technology">
    <meta property="og:url" content="https://articles.engineers.my/">
    <meta property="og:site_name" content="EngineersMY Articles">
    <meta property="og:type" content="website">

  </head>

  <body>

<nav class="navbar navbar-inverse bg-primary">

  <a href="/" class="navbar-brand">
    EngineersMY Articles
  </a>

</nav>

  <div class="mw7 center ph3 pv2">
    <h1 class="f3 lh-title b mb3">Setup DataCenter In a Box Using LXD &#43; ZFS</h1>
    <div class="flex justify-between grey-3">
      <p>Wed, Jun 21, 2017</p>
      <p>Read in 9 minutes</p>
    </div>
    <div class="cms mw6">
      <p>To document a method to deploy the so that can simulate a full VNet like in Azure Cloud system on your own laptop.</p>
      
      

<h1 id="setup-datacenter-in-a-box-using-lxd-zfs">Setup DataCenter In a Box Using LXD + ZFS</h1>

<h2 id="tl-dr">TL;DR</h2>

<p>A simple method to setup a clustered environment mimicking your production system without needing to spend a dime in cloud computing cost in Azure.  Setup a full multi-VNet with 3 subnets in which to deploy 3 independent Consul node to form a fully running Consul cluster.</p>

<h3 id="getting-started">Getting Started</h3>

<p>Assumes on one of the below environments running Ubuntu 16.04:
- Xhyve (OSX)
- Hyper-V (Windows)
- VirtualBox (no native Hypervisors like above)
- Azure (simulate full cluster using only one node; with fast network)</p>

<p>Get the script, make executable and run the script as found in the Nomad Box <a href="https://github.com/leowmjw/nomad-box">repo</a>. The main script inside the repo is in <code>/lxd/scripts/initlxd.sh</code>.</p>

<p>The following is just explanation what the script is doing&hellip;</p>

<h3 id="installing-lxd-zfs">Installing LXD + ZFS</h3>

<p>The packages from the standard Ubuntu 16.04 is just too old so there is a need to get the latest packages (which has the important features like emulating VNet-like network + sub-networks):</p>

<pre><code># LXD needs the bleeding edge; thus add as per below to get at LXD + ZFS
export DEBIAN_FRONTEND=noninteractive &amp;&amp; \
    apt-get install -y software-properties-common python-software-properties &amp;&amp; \
    add-apt-repository ppa:ubuntu-lxc/lxd-git-master &amp;&amp; \
    apt-get update &amp;&amp; apt-get install -y lxd zfsutils-linux

</code></pre>

<p>To confirm the installation for LXD + ZFS is done correctly and using the latest version:</p>

<pre><code>$ lxc --version
2.14

$ sudo zpool list
no pools available
</code></pre>

<h3 id="setup-lxd-zfs">Setup LXD + ZFS</h3>

<p>The setup is automated by executing the <code>/tmp/script/initlxd.sh</code> script.
Below is the in-depth explanation of the why and how the script is structured.</p>

<h4 id="zfs-setup">ZFS Setup</h4>

<p>For a development type setup, it is fine to use a file loopback as the backing for the ZFS setup.  In real production, as fast SSD partition and device would be advisable.</p>

<p>For a standalone VirtualBox, Xhyve or Hyper-V installation; the below would be the code to prepare the ZFS device to be made into a ZPool:</p>

<pre><code>$ # On a VBox, the only valid assumption is that it will have a /var/lib/lxd/ directory after the installation of LXD
$ sudo dd if=/dev/zero of=/var/lib/lxd/zfs.img bs=1k count=1 seek=100M &amp;&amp; \
        zpool create my-zfs-pool /var/lib/lxd/zfs.img

</code></pre>

<p>On an Azure machine; the standard <code>Standard_DS2_v2_Promo</code> instance looks like below:</p>

<pre><code>$ sudo df -TH
Filesystem     Type      Size  Used Avail Use% Mounted on
udev           devtmpfs  3.7G     0  3.7G   0% /dev
tmpfs          tmpfs     731M   18M  713M   3% /run
/dev/sda1      ext4       32G  2.7G   29G   9% /
tmpfs          tmpfs     3.7G     0  3.7G   0% /dev/shm
tmpfs          tmpfs     5.3M     0  5.3M   0% /run/lock
tmpfs          tmpfs     3.7G     0  3.7G   0% /sys/fs/cgroup
/dev/sda15     vfat      110M  3.6M  106M   4% /boot/efi
/dev/sdb1      ext4       15G   42M   14G   1% /mnt
tmpfs          tmpfs     731M     0  731M   0% /run/user/1000

$ free
              total        used        free      shared  buff/cache   available
Mem:        7131864      341656     5112716       17364     1677492     6483948
Swap:             0           0           0
</code></pre>

<p>As can be seen above, the root device (/dev/sda1), there is 29G available space.  The <strong>ephemeral</strong> device is currently not being used as swap (see the <code>free</code> command output) so can be used as a ZFS cache.</p>

<p>The code to setup the Azure node is as below:</p>

<pre><code># Code:
#     dd if=/dev/zero of=/var/lib/lxd/zfs-azure.img bs=1k count=1 seek=100M &amp;&amp; \
#          zpool create my-zfs-pool /var/lib/lxd/zfs-azure.img &amp;&amp; \
#          umount /mnt &amp;&amp; zpool add -f my-zfs-pool cache /dev/sdb
$ AZURE_MODE=&quot;x&quot; sudo ./test.bash 
1+0 records in
1+0 records out
1024 bytes (1.0 kB, 1.0 KiB) copied, 9.9e-05 s, 10.3 MB/s

$ df -TH
Filesystem     Type      Size  Used Avail Use% Mounted on
udev           devtmpfs  3.7G     0  3.7G   0% /dev
tmpfs          tmpfs     731M   18M  713M   3% /run
/dev/sda1      ext4       32G  2.7G   29G   9% /
tmpfs          tmpfs     3.7G     0  3.7G   0% /dev/shm
tmpfs          tmpfs     5.3M     0  5.3M   0% /run/lock
tmpfs          tmpfs     3.7G     0  3.7G   0% /sys/fs/cgroup
/dev/sda15     vfat      110M  3.6M  106M   4% /boot/efi
/dev/sdb1      ext4       15G   42M   14G   1% /mnt
tmpfs          tmpfs     731M     0  731M   0% /run/user/1000
my-zfs-pool    zfs       104G     0  104G   0% /my-zfs-pool
</code></pre>

<p>The previous device will start losing data if it gets corrupted so is not advisable for production; at the minimum there should be a mirror.</p>

<p>So if we wanted to do a mirror, it can be mirrored with the smaller device for 10GB let&rsquo;s say, like so:</p>

<pre><code>dd if=/dev/zero of=/var/lib/lxd/zfs-mirror1.img bs=1k count=1 seek=10M &amp; \
    dd if=/dev/zero of=/var/lib/lxd/zfs-mirror2.img bs=1k count=1 seek=10M &amp;&amp; \
    zpool create my-zfs-pool mirror /var/lib/lxd/zfs-mirror1.img /var/lib/lxd/zfs-mirror2.img 
</code></pre>

<p>Once the underlying pool is setup; you can confirm its sturcture and even run <code>zpool scrub</code> to check the underlying setup has no errors.</p>

<pre><code>$ sudo zpool status
  pool: my-zfs-pool
 state: ONLINE
  scan: none requested
config:

	NAME                              STATE     READ WRITE CKSUM
	my-zfs-pool                       ONLINE       0     0     0
	  mirror-0                        ONLINE       0     0     0
	    /var/lib/lxd/zfs-mirror1.img  ONLINE       0     0     0
	    /var/lib/lxd/zfs-mirror2.img  ONLINE       0     0     0

errors: No known data errors
</code></pre>

<p>For even better performance and redundancies; go with RaidZ setup; will be left as an exercise for the reader (Quiz time!)</p>

<h4 id="lxd-headless-setup">LXD Headless Setup</h4>

<p>Now that the underlying storage pool is ready, a headless initialization of LXD can be done by executing the below:</p>

<pre><code>cat &lt;&lt;EOF | lxd init --verbose --preseed
# Daemon settings
config:
  core.https_address: 0.0.0.0:9999
  core.trust_password: passw0rd
  images.auto_update_interval: 36

# Storage pools
storage_pools:
- name: data
  driver: zfs
  config:
    source: my-zfs-pool/my-zfs-dataset

# Network devices
networks:
- name: lxd-my-bridge
  type: bridge
  config:
    ipv4.address: auto
    ipv6.address: none

# Profiles
profiles:
- name: default
  devices:
    root:
      path: /
      pool: data
      type: disk
- name: test-profile
  description: &quot;Test profile&quot;
  config:
    limits.memory: 2GB
  devices:
    test0:
      name: test0
      nictype: bridged
      parent: lxd-my-bridge
      type: nic
EOF

</code></pre>

<p>Finally the needed 3 subnets can be created:</p>

<pre><code>$ lxc network create fsubnet1 ipv6.address=none ipv4.address=10.1.1.1/24 ipv4.nat=true
Network fsubnet1 created

$ lxc network create fsubnet2 ipv6.address=none ipv4.address=10.1.2.1/24 ipv4.nat=true
Network fsubnet2 created

$ lxc network create fsubnet3 ipv6.address=none ipv4.address=10.1.3.1/24 ipv4.nat=true
Network fsubnet3 created

$ lxc network list
+---------------+----------+---------+-------------+---------+
|     NAME      |   TYPE   | MANAGED | DESCRIPTION | USED BY |
+---------------+----------+---------+-------------+---------+
| docker0       | bridge   | NO      |             | 0       |
+---------------+----------+---------+-------------+---------+
| eth0          | physical | NO      |             | 0       |
+---------------+----------+---------+-------------+---------+
| fsubnet1      | bridge   | YES     |             | 0       |
+---------------+----------+---------+-------------+---------+
| fsubnet2      | bridge   | YES     |             | 0       |
+---------------+----------+---------+-------------+---------+
| fsubnet3      | bridge   | YES     |             | 0       |
+---------------+----------+---------+-------------+---------+
| lxd-my-bridge | bridge   | YES     |             | 0       |
+---------------+----------+---------+-------------+---------+

</code></pre>

<h3 id="auto-install-consul-cluster">Auto Install Consul Cluster</h3>

<p>Now with the underlying network setup, the goal is to auto boot up nodes and form a Consul Cluster automatically on an Ubuntu 17.04 installation with the latest Consul 0.8.4:</p>

<p>The available image repos:</p>

<pre><code>$ lxc remote list

+-----------------+------------------------------------------+---------------+--------+--------+
|      NAME       |                   URL                    |   PROTOCOL    | PUBLIC | STATIC |
+-----------------+------------------------------------------+---------------+--------+--------+
| images          | https://images.linuxcontainers.org       | simplestreams | YES    | NO     |
+-----------------+------------------------------------------+---------------+--------+--------+
| local (default) | unix://                                  | lxd           | NO     | YES    |
+-----------------+------------------------------------------+---------------+--------+--------+
| ubuntu          | https://cloud-images.ubuntu.com/releases | simplestreams | YES    | YES    |
+-----------------+------------------------------------------+---------------+--------+--------+
| ubuntu-daily    | https://cloud-images.ubuntu.com/daily    | simplestreams | YES    | YES    |
+-----------------+------------------------------------------+---------------+--------+--------+
</code></pre>

<p>We&rsquo;ll get the latest Ubuntu images from the remote ubuntu-daily to live on the leading edge.  Images that come from the cloud-images repo has one important characteristic; it has <code>cloud-init</code> which we&rsquo;ll need to bootstrap the Consul nodes into the cluster.</p>

<p>Image can be obtained by copying image from remote; giving it an alias:</p>

<pre><code># Pull 17.04 and latest; have options to select ..
$ lxc image copy ubuntu-daily:17.04 local: --alias=zesty
$ lxc image list
+-------+--------------+--------+---------------------------------------+--------+----------+------------------------------+
| ALIAS | FINGERPRINT  | PUBLIC |              DESCRIPTION              |  ARCH  |   SIZE   |         UPLOAD DATE          |
+-------+--------------+--------+---------------------------------------+--------+----------+------------------------------+
| zesty | b334ae64a8c3 | no     | ubuntu 17.04 amd64 (daily) (20170617) | x86_64 | 158.53MB | Jun 17, 2017 at 3:37pm (UTC) |
+-------+--------------+--------+---------------------------------------+--------+----------+------------------------------+
$ lxc image info zesty
Fingerprint: b334ae64a8c3a6f873509a85efdcf2786e3b7fb4c4b20869327bbbf586042bfe
Size: 158.53MB
Architecture: x86_64
Public: no
Timestamps:
    Created: 2017/06/17 00:00 UTC
    Uploaded: 2017/06/17 15:37 UTC
    Expires: 2018/01/25 00:00 UTC
    Last used: 2017/06/17 15:37 UTC
Properties:
    serial: 20170617
    version: 17.04
    architecture: amd64
    description: ubuntu 17.04 amd64 (daily) (20170617)
    label: daily
    os: ubuntu
    release: zesty
Aliases:
    - zesty (zesty)
Auto update: enabled
Source:
    Server: https://cloud-images.ubuntu.com/daily
    Protocol: simplestreams
    Alias: 17.04
</code></pre>

<h4 id="executing-cloud-init-templates">Executing cloud-init templates</h4>

<p>Now that the images are available, there is a need to create a profile for the Foundation type nodes which will execute the cloud-init script via the user-data parameter.</p>

<p>The cloud-init script will be similar to the ones used in the main Nomad Box Azure setup.  The lxd-foundation-init.sh used below can be found in the Nomad Box repo under <code>/lxd/scripts/lxd-foundation-init.sh</code></p>

<pre><code># Attach profile for Foundation-type nodes
lxc profile create foundation
# Need to provide the cloud-init.sh scripts ..
lxc profile set foundation user.user-data - &lt; /tmp/script/lxd-foundation-init.sh

</code></pre>

<p>Once the correct profile is setup as per above, the nodes can be initialized and started:</p>

<pre><code># Exec in and confirm it is running
lxc init zesty -p default -p foundation f1 &amp;&amp; \
    lxc network attach fsubnet1 f1 eth0 &amp;&amp; \
    lxc config device set f1 eth0 ipv4.address 10.1.1.4

lxc init zesty -p default -p foundation f2 &amp;&amp; \
    lxc network attach fsubnet2 f2 eth0 &amp;&amp; \
	lxc config device set f2 eth0 ipv4.address 10.1.2.4

lxc init zesty -p default -p foundation f3 &amp;&amp; \
    lxc network attach fsubnet3 f3 eth0 &amp;&amp; \
	lxc config device set f3 eth0 ipv4.address 10.1.3.4

lxc start f1 &amp;&amp; lxc start f2 &amp;&amp; lxc start f3

</code></pre>

<p>NOTE: Consul servers need to have custom node-id inside LXD, otherwise it gets confused that there are multiple Consul servers that are identical.</p>

<p>To solve this, the Consul servers need to be executed with the flag <code>-disable-host-node-id</code> as documented <a href="https://www.consul.io/docs/agent/options.html#_disable_host_node_id">here</a></p>

<pre><code>root@f3:/opt/consul# echo ${CONSUL_BIND_ADDRESS}
10.1.3.4
root@f3:/opt/consul# /opt/consul/consul agent -server -ui -bootstrap-expect=3 -data-dir=/tmp/consul   -config-dir=/opt/consul/consul.d -retry-join=10.1.1.4 -retry-join=10.1.2.4 -retry-join=10.1.3.4 -bind=${CONSUL_BIND_ADDRESS} -disable-host-node-id

</code></pre>

<h3 id="output">Output</h3>

<p>For VirtualBox, you&rsquo;ll need to use sshuttle to access the LXD nodes network space of <code>10.1.0.0/16</code> with a command like below:</p>

<pre><code># Replace &lt;PUBLIC_IP_BASTION&gt;; for Azure it might be 52.187.116.82, VBox might be 192.168.1.132 etc
$ sshuttle -vH -r testadmin@&lt;PUBLIC_IP_BASTION&gt; 10.1.0.0/16

</code></pre>

<p>When you go to the IP address or node-name (f1,f2,f3) of any of the LXD nodes (<a href="http://10.1.1.4">http://10.1.1.4</a> [f1] or <a href="http://10.1.2.4">http://10.1.2.4</a> [f2] or <a href="http://10.1.3.4">http://10.1.3.4</a> [f3] )with Consul server and at Consul port, you will see the following:</p>

<p><img src="/img/Consul-Admin.png" alt="Consul Admin Dashboard" /></p>

<p>Other commands can be executed against any of the Consul servers to ensure the quorum is there:</p>

<pre><code>testadmin@acme-nomad-dev-experiment-node-1:~$ CONSUL_HTTP_ADDR=http://10.1.3.4:8500 /opt/consul/consul members
Node  Address        Status  Type    Build  Protocol  DC
f1    10.1.1.4:8301  alive   server  0.8.4  2         dc1
f2    10.1.2.4:8301  alive   server  0.8.4  2         dc1
f3    10.1.3.4:8301  alive   server  0.8.4  2         dc1
</code></pre>

<p>Now, you have a usable Consul Server Cluster, next is to add more nodes that will do the load balancer(Director Nodes) and execute workloads (Worker Node).  That will be in the next article ..</p>

    </div>
  </div>
<footer class="bg-black ph3 pv4 white">

	<div class="mw7 center pt3">

		<div class="flex-ns justify-between">

			<div>
				<h3 class="f4 b lh-title mb2 primary">Social media</h3>
				<ul class="mhn2">
					<li class="dib ph2 raise">
  <a href="https://github.com/engineersmy" class="link bg-white black db relative br-100 pa2">
    <svg width="16px" height="16px" class="db">
      <use xlink:href="#twitter"></use>
    </svg>
  </a>
</li>

				</ul>
			</div>

		</div>
	</div>

</footer>

<div style="height: 0; width: 0; position: absolute; visibility: hidden">
	
  <svg xmlns="http://www.w3.org/2000/svg"><symbol id="Facebook" viewBox="0 0 32 32"><path d="M30.7 0H1.3C.6 0 0 .6 0 1.3v29.3c0 .8.6 1.4 1.3 1.4H17V20h-4v-5h4v-4c0-4.1 2.6-6.2 6.3-6.2 1.8 0 3.3.2 3.7.2v4.3h-2.6c-2 0-2.5 1-2.5 2.4V15h5l-1 5h-4l.1 12h8.6c.7 0 1.3-.6 1.3-1.3V1.3C32 .6 31.4 0 30.7 0z"/></symbol><symbol id="instagram" viewBox="0 0 32 32"><path d="M28.2 0H3.8C1.7 0 0 1.7 0 3.8v24.4C0 30.3 1.7 32 3.8 32h24.4c2.1 0 3.8-1.7 3.8-3.8V3.8C32 1.7 30.3 0 28.2 0zM24 4h3c.6 0 1 .4 1 1v3c0 .6-.4 1-1 1h-3c-.6 0-1-.4-1-1V5c0-.6.4-1 1-1zm-8 5.9c3.4 0 6.2 2.7 6.2 6.1 0 3.4-2.8 6.1-6.2 6.1-3.4 0-6.2-2.7-6.2-6.1.1-3.4 2.8-6.1 6.2-6.1zM28 29H4c-.6 0-1-.4-1-1V13h4c-.5.8-.7 2.1-.7 3 0 5.4 4.4 9.7 9.7 9.7 5.4 0 9.7-4.4 9.7-9.7 0-.9-.1-2.3-.8-3h4v15c.1.6-.3 1-.9 1z"/></symbol><symbol id="twitter" viewBox="0 0 32 32"><path d="M32 6.1c-1.2.5-2.4.9-3.8 1 1.4-.8 2.4-2.1 2.9-3.6-1.3.8-2.7 1.3-4.2 1.6C25.7 3.8 24 3 22.2 3c-3.6 0-6.6 2.9-6.6 6.6 0 .5.1 1 .2 1.5-5.5-.3-10.3-2.9-13.6-6.9-.6 1-.9 2.1-.9 3.3 0 2.3 1.2 4.3 2.9 5.5-1.1 0-2.1-.3-3-.8v.1c0 3.2 2.3 5.8 5.3 6.4-.6.1-1.1.2-1.7.2-.4 0-.8 0-1.2-.1.8 2.6 3.3 4.5 6.1 4.6-2.2 1.8-5.1 2.8-8.2 2.8-.5 0-1.1 0-1.6-.1 3 1.8 6.5 2.9 10.2 2.9 12.1 0 18.7-10 18.7-18.7v-.8c1.2-1 2.3-2.1 3.2-3.4z"/></symbol><symbol id="vimeo" viewBox="0 0 32 32"><path d="M32 8.6c-.1 3.1-2.3 7.4-6.5 12.8-4.4 5.7-8 8.5-11 8.5-1.9 0-3.4-1.7-4.7-5.2-.9-3.2-1.7-6.3-2.6-9.5-1-3.4-2-5.2-3.1-5.2-.2 0-1.1.5-2.5 1.5L0 9.6c1.6-1.4 3.1-2.8 4.7-4.2 2.1-1.8 3.7-2.8 4.7-2.9 2.5-.2 4 1.5 4.6 5.1.6 3.9 1.1 6.4 1.3 7.3.7 3.3 1.5 4.9 2.4 4.9.7 0 1.7-1.1 3-3.2s2.1-3.7 2.2-4.8c.2-1.8-.5-2.7-2.2-2.7-.8 0-1.6.2-2.4.5 1.6-5.2 4.6-7.7 9-7.5 3.3.2 4.9 2.3 4.7 6.5z"/></symbol></svg>
  
</div>


<script src="/app.js"></script>

</body>

</html>

